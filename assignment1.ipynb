{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backpropagation\n",
    "\n",
    "# classify digits 0-9 (28x28 pixels) using ANN\n",
    "# x is collection of N stacked input images 784*N\n",
    "# y is [0,1]^(10xN) collection of N input images\n",
    "# y_i,j (digit i, image j)\n",
    "\n",
    "# t:  iternation index of gradient descent\n",
    "# γ:  learning rate of gradient descent\n",
    "# Wk: matrix of weights at layer k \n",
    "# bk: vector of biases at layer k\n",
    "# x:  matrix with the N training input vectors in columns\n",
    "# hk=gk(ak):    matrix with all hidden outputs at layer k in columns\n",
    "# ak=Wkhk−1+bk: matrix with all weighted sums at layer k in columns\n",
    "# gk: activation function at layerk,\n",
    "# 1N: column vector of size N containing only ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read MNIST Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import MNISTtools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function load in module MNISTtools:\n",
      "\n",
      "load(dataset='training', path=None)\n",
      "    Import either the training or testing MNIST data set.\n",
      "    It returns a pair with the first element being the collection of\n",
      "    images stacked in columns and the second element being a vector\n",
      "    of corresponding labels from 0 to 9.\n",
      "    \n",
      "    Arguments:\n",
      "        dataset (string, optional): either \"training\" or \"testing\".\n",
      "            (default: \"training\")\n",
      "        path (string, optional): the path pointing to the MNIST dataset\n",
      "            If path=None, it looks succesively for the dataset at:\n",
      "            '/datasets/MNIST' and './MNIST'. (default: None)\n",
      "    \n",
      "    Example:\n",
      "        x, lbl = load(dataset=\"testing\", path=\"/Folder/for/MNIST\")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(MNISTtools.load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function show in module MNISTtools:\n",
      "\n",
      "show(image)\n",
      "    Render a given MNIST image provided as a column vector.\n",
      "    \n",
      "    Arguments:\n",
      "        image (array): an array of shape (28*28) or (28, 28) representing a\n",
      "            grey level image of size 28 x 28. Values are expected to be in the\n",
      "            range [0, 1].\n",
      "    \n",
      "    Example:\n",
      "        x, lbl = load(dataset=\"training\", path=\"/datasets/MNIST\")\n",
      "        show(x[:, 0])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(MNISTtools.show)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Images stacked in columns\n",
    "# Vector of corresponding labels from 0 to 9\n",
    "xtrain, ltrain = MNISTtools.load(\"training\", path=\"./mnist/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "784\n",
      "60000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "(row,col) = xtrain.shape\n",
    "print(row) \n",
    "print(col)\n",
    "plt.imshow(xtrain[:,42].reshape(28,28),cmap='gray')\n",
    "plt.show()\n",
    "print(ltrain[42])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "255\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(np.min(xtrain))\n",
    "print(np.max(xtrain))\n",
    "print(type(xtrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "def normalize_MNIST_images(x):\n",
    "    x = (x-255.0/2)/(255.0/2)\n",
    "    x.astype(np.float32)\n",
    "    return x\n",
    "\n",
    "print(np.min(xtrain))\n",
    "print(np.max(xtrain))\n",
    "xtrain = normalize_MNIST_images(xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "def label2onehot(lbl):\n",
    "    d = np.zeros((lbl.max() + 1, col))\n",
    "    d[lbl, np.arange(lbl.size)] = 1\n",
    "    return d\n",
    "\n",
    "dtrain = label2onehot(ltrain)\n",
    "#validate\n",
    "print(dtrain[:,42])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "def onehot2label(d):\n",
    "    lbl = d.argmax(axis=0)\n",
    "    return lbl\n",
    "\n",
    "label = onehot2label(dtrain[:,42])\n",
    "#validate\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When using exponential functions, make sure inputs aren't too large or may observe numerial issues (Inf, NaN). Softmax function uses exponentials to get rid of this problem.\n",
    "def softmax(a):\n",
    "    M = a.max(axis=0)\n",
    "    y = exp(a-M)/exp(a-M).sum(axis=0)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmaxp is softmax prime\n",
    "# e: vector given during backward propagation\n",
    "def softmaxp(a, e):\n",
    "    g = softmax(a)\n",
    "    for i in range(col):\n",
    "        d[:,i] = g[:,i]*e[:,i]-np.dot(g[:,i],e[:,i])*g[:,i]\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps         = 1e-6# finite difference step\n",
    "a           = np.random.randn(10, 200)# random inputs\n",
    "e           = np.random.randn(10, 200)# random directions\n",
    "diff        = softmaxp(a, e)\n",
    "diff_approx = COMPLETE\n",
    "rel_error   = np.abs(diff - diff_approx).mean() / np.abs(diff_approx).mean()\n",
    "print(rel_error,'should be smaller than 1e-6')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# He and Xavier initializations\n",
    "def init_shallow(Ni, Nh, No):\n",
    "    b1 = np.random.randn(Nh, 1)   / np.sqrt((Ni+1.)/2.)\n",
    "    W1 = np.random.randn(Nh, Ni)  / np.sqrt((Ni+1.)/2.)\n",
    "    b2 = np.random.randn(No, 1)   / np.sqrt((Nh+1.))\n",
    "    W2 = np.random.randn(No, Nh)  / np.sqrt((Nh+1.))\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "Ni = xtrain.shape[0]\n",
    "Nh = 64\n",
    "No = dtrain.shape[0]\n",
    "netinit = init_shallow(Ni, Nh, No)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forwardprop_shallow(x, net):\n",
    "    W1 = net[0]\n",
    "    b1 = net[1]\n",
    "    W2 = net[2]\n",
    "    b2 = net[3]\n",
    "    a1 = W1.dot(x) + b1\n",
    "    COMPLETE\n",
    "    return y \n",
    "\n",
    "yinit = forwardprop_shallow(xtrain, netinit)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
